
# 分布式系统
Dustin是一个开源的软件开发者，同时也是Mozilla的一名发布工程师。他参与的项目包括在Puppet中配置主机系统，一个基于Flask的Web框架，为防火墙配置做单元测试，还有一个在Twisted Python下开发的持续集成系统框架。你可以通过[GitHub](https://github.com/djmitche)或者<dustin@mozillar.com>联系他。
## 介绍
在这一章，我们将会一起探索如何实现一个网络协议用于可靠的分布式计算。正确实现一个网络协议并不简单，因此我们会采用一些技巧来尽可能的减少、查找和修复漏洞。要建立一个可靠地软件，同样需要一些特别的开发和调试技巧。
## 情景思考
这一章的重点在于网络协议的实现，但是首先让我们以简单的银行账户管理服务为例做一个思考。在这个服务中，每一个账户都有一个当前余额，同时每个账户都有自己的账号。用户可以通过“存款”、“转账”、“查询当前余额”等操作来连接账户。“转账”操作同时涉及了两个账户——转出账户和转入账户——并且如果账户余额不足，转账操作必须被驳回。  
  
如果这个服务仅仅在一个服务器上部署，很容易就能够实现：使用一个操作锁来确保“转账”操作不会同时进行，同时对转出账户的进行校验。然而，银行不可能仅仅依赖于一个服务器来储存账户余额这样的关键信息，通常，这些服务都是被分布在多个服务器上的，每一个服务器各自运行着相同代码的实例。用户可以通过任何一个服务器来操作账户。  
  
在一个简单的分布式处理系统的实现中，每个服务器都会保存一份账户余额的副本。它会处理任何收到的操作，并且将账户余额的更新发送给其他的服务器。但是这种方法有一个严重的问题：如果两个服务器同时对一个账户进行操作，哪一个新的账户余额是正确的？即使服务器不共享余额而是共享操作，对一个账户同时进行转账操作也可能造成透支。  
  
从根本上来说，这些错误的发生都是由于服务器使用它们本地状态来响应操作，而不是首先确保本地状态与其他服务器相匹配。比如，想象服务器A接到了从账号101向账号202转账的操作指令，而此时服务器B已经处理了另一个把账号101的钱都转到账号202的请求，却没有通知服务器A。这样，服务器A的本地状态与服务器B不一样，即使会造成账户101透支，服务器A依然允许从账号101进行转账操作。  
## 分布式状态机
为了防止上述情况发生我们采用了一种叫做“分布式状态机”的工具。它的思路是对每个同样的输入，每个服务器都运行同样的对应的状态机。由于状态机的特性，对于同样的输入每个服务器的输出都是一样的。对于像“转账”、“查询当前余额”等操作，账号和余额也都是状态机的输入。  
  
这个应用的状态机比较简单：
```python
 def execute_operation(state, operation):
     if operation.name == 'deposit':
         if not verify_signature(operation.deposit_signature):
         return state, False
         state.accounts[operation.destination_account] += operation.amount
         return state, True
     elif operation.name == 'transfer':
         if state.accounts[operation.source_account] < operation.amount:
             return state, False
             state.accounts[operation.source_account] -= operation.amount
         state.accounts[operation.destination_account] += operation.amount
         return state, True
     elif operation.name == 'get-balance':
     return state, state.accounts[operation.account]
 ```
值得注意的是，运行“查询当前余额”操作时虽然并不会改变当前状态，但是我们依然把它当做一个状态变化操作来实现。这确保了返回的余额是分布式系统中的最新信息，并且不是基于一个服务器上的本地状态来进行返回的。  
  
这可能跟你在计算机课程中学习到的典型的状态机不太一样。传统的状态机是一系列有限个状态的集合，每个状态都与一个标记的转移行为相对应，而在本文中，状态机的状态是账户余额的集合，因此存在无穷多个可能的状态。但是，状态机的基本规则同样适用于本文的状态机：对于同样的初始状态，同样的输入总是有同样的输出。  
  
因此，分布式状态机确保了对于同样的操作，每个主机都会有同样的相应。但是，为了确保每个服务器都允许状态机的输入，前文中提到的问题依然存在。这是一个一致性问题，为了解决它我们采用了一种派生的Paxos算法。  
### Multi-Paxos算法

就单个静态值达成共识并不是特别有用。诸如银行账户服务之类的分布式系统希望的是就某一特定状态（账户余额）随时更新。我们使用Paxos算法就每个操作达成一致，将其视为状态机转换。

实际上，Multi-Paxos是一系列简单的Paxos实例（插槽），每个实例都按顺序编号。每个状态转换都被赋予“实例编号”，并且集群的每个成员以严格的数字顺序执行转换。要更改集群的状态（例如处理转账操作），我们需要在下一个实例中就该操作达成共识。具体而言，这意味着为每个消息添加一个实例号，并且每个实例都跟踪所有协议状态。

每个实例运行一次Paxos最少需要两次往返，效率太低。Multi-Paxos通过对所有实例使用相同的一组提案编号进行优化，并一次为所有实例执行Prepare/ Promise阶段。

### Paxos算法很难实现

在实用软件中实现Multi-Paxos是非常困难的，它催生了许多论文，比如模仿Lamport的“Paxos Made Simple”的“Paxos Made Practical”。

首先，上述多个提议者问题在繁忙的环境中可能会出现故障，因为每个集群成员试图在每个实例中确定其状态机操作。解决方法是选出一个负责为每个时段提交选票的“领导者”。然后，所有其他集群节点将新操作发送给领导者以供执行。因此，在仅有一个领导者的正常操作中，不会发生选票冲突。

Prepare/ Promise阶段可以作为一种领导者选举：无论哪个集群成员拥有最近通过决议的投票编码，都被视为领导者。然后领导者可以直接执行Accept/Accepted阶段而无需重复第一阶段。我们将在后面看到，领导者选举实际上非常复杂。

虽然简单的Paxos算法保证集群不会达成冲突的决议，但它不能保证做出任何决议。假如初始Prepare消息丢失，则Proposer将永远不会等到Promise消息。解决这个问题需要精心策划的重新传输：足以最终取得进展，但不会太多以至于集群在数据包风暴中淹没。

另一个问题是传播决议。对于正常情况， Decision消息的简单广播可以处理这种情况。但是，如果消息丢失，则节点可能永远不知道该决议，并且无法为稍后的实例执行状态机转换。因此，协议实现还需要一些机制来共享有关决议的信息。

我们对分布式状态机的使用提出了另一个有趣的挑战：启动。当一个新节点启动时，它需要赶上集群的现有状态。虽然它可以通过追赶自第一个实例以来的所有实例的决议来实现，但在一个成熟的集群中，这可能涉及数百万个实例。此外，我们还需要一些方法来初始化一个新的集群。

理论和算法说得够多了，还是让我们来看看代码吧。

## 分布式系统介绍

本章中的*Cluster*库实现了一种简单形式的Multi-Paxos算法。它被设计为一个库，以为更大的应用程序提供一致性服务。

这个库的用户将依赖于它的正确性，因此构造代码非常重要，这样我们就可以看到并测试它与规范的对应关系。如此复杂的协议可能会出现奇怪的故障，因此我们将为重现和调试故障提供支持。

本章的实现是概念验证代码：足以证明核心概念是实用的，但没有生产所需的所有普通设备。代码的结构化使得以后可以在对核心代码进行最小更改的情况下添加此类设备。

让我们开始吧。

### 类型和常量

Cluster的协议使用十五种不同的消息类型，每种类型都定义为Python [`namedtuple`](https://docs.python.org/3/library/collections.html) 。

```python
    Accepted = namedtuple('Accepted', ['slot', 'ballot_num'])
    Accept = namedtuple('Accept', ['slot', 'ballot_num', 'proposal'])
    Decision = namedtuple('Decision', ['slot', 'proposal'])
    Invoked = namedtuple('Invoked', ['client_id', 'output'])
    Invoke = namedtuple('Invoke', ['caller', 'client_id', 'input_value'])
    Join = namedtuple('Join', [])
    Active = namedtuple('Active', [])
    Prepare = namedtuple('Prepare', ['ballot_num'])
    Promise = namedtuple('Promise', ['ballot_num', 'accepted_proposals'])
    Propose = namedtuple('Propose', ['slot', 'proposal'])
    Welcome = namedtuple('Welcome', ['state', 'slot', 'decisions'])
    Decided = namedtuple('Decided', ['slot'])
    Preempted = namedtuple('Preempted', ['slot', 'preempted_by'])
    Adopted = namedtuple('Adopted', ['ballot_num', 'accepted_proposals'])
    Accepting = namedtuple('Accepting', ['leader'])
```

使用命名元组来描述每种消息类型可以使代码保持整洁，并有助于避免一些简单的错误。如果未给出完全正确的属性，则命名元组的构造函数将引发异常，这有助于发现拼写错误。元组在日志消息中可以很好地格式化，另外相对于字典来说它使用更少的内存。

创建易读的消息：

```python
    msg = Accepted(slot=10, ballot_num=30)
```

并且只需最少的额外输入即可访问该消息的字段：

```python
    got_ballot_num = msg.ballot_num
```

我们将在后续章节中看到这些消息的含义。此外还引入了一些常量，其中大多数定义了各种消息的超时：

```python
    JOIN_RETRANSMIT = 0.7
    CATCHUP_INTERVAL = 0.6
    ACCEPT_RETRANSMIT = 1.0
    PREPARE_RETRANSMIT = 1.0
    INVOKE_RETRANSMIT = 0.5
    LEADER_TIMEOUT = 1.0
    NULL_BALLOT = Ballot(-1, -1)  # sorts before all real ballots
    NOOP_PROPOSAL = Proposal(None, None, None)  # no-op to fill otherwise empty slots
```

最后，Cluster使用两个量对应协议描述的数据类型：

```python
    Proposal = namedtuple('Proposal', ['caller', 'client_id', 'input'])
    Ballot = namedtuple('Ballot', ['n', 'leader'])
```

### 组件模型

基于人类脑容量的限制，我们无法立即推断整个集群的实施 - 它太长了，所以很容易错过细节。出于类似的原因，大型单片代码库很难测试：测试用例必须操纵许多脆弱的移动部件，对代码进行任何更改都容易造成崩溃。

为了鼓励可测试性并保持代码可读，我们将Cluster分解为与协议中描述的角色相对应的少数几个类。每个都是Role的子类。

```python
class Role(object):

    def __init__(self, node):
        self.node = node
        self.node.register(self)
        self.running = True
        self.logger = node.logger.getChild(type(self).__name__)

    def set_timer(self, seconds, callback):
        return self.node.network.set_timer(self.node.address, seconds,
                                           lambda: self.running and callback())

    def stop(self):
        self.running = False
        self.node.unregister(self)
```

集群节点所具有的角色由Node类结合在一起，Node类表示网络上的单个节点。随着程序的进行，角色会添加到节点中或从节点中删除。到达节点的消息将中继到所有活动角色，调用“do_ + 消息类型名”的方法。这些do方法接收消息的属性作为关键字参数，以便于访问。Node类还提供了一个方便的send方法调用，其使用functools.partial为Network类的相同方法提供一些参数。

```python
class Node(object):
    unique_ids = itertools.count()

    def __init__(self, network, address):
        self.network = network
        self.address = address or 'N%d' % self.unique_ids.next()
        self.logger = SimTimeLogger(
            logging.getLogger(self.address), {'network': self.network})
        self.logger.info('starting')
        self.roles = []
        self.send = functools.partial(self.network.send, self)

    def register(self, roles):
        self.roles.append(roles)

    def unregister(self, roles):
        self.roles.remove(roles)

    def receive(self, sender, message):
        handler_name = 'do_%s' % type(message).__name__

        for comp in self.roles[:]:
            if not hasattr(comp, handler_name):
                continue
            comp.logger.debug("received %s from %s", message, sender)
            fn = getattr(comp, handler_name)
            fn(sender=sender, **message._asdict())
    
```

### 应用接口

应用程序在每个集群成员上创建并启动一个Member对象，提供该应用的状态机和一个对等列表。如果成员加入现有集群，则Member对象会向节点添加bootstrap角色，如果正在创建新集群，则添加seed。它在一个单独的线程中运行协议（通过`Network.run` ）。

应用通过invoke方法与集群交互， 由invoke方法启动状态转换的提议。一旦确定了该提议并且状态机运行， invoke将返回机器的输出。该方法使用简单的同步队列来等待协议线程的结果。

```python
class Member(object):

    def __init__(self, state_machine, network, peers, seed=None,
                 seed_cls=Seed, bootstrap_cls=Bootstrap):
        self.network = network
        self.node = network.new_node()
        if seed is not None:
            self.startup_role = seed_cls(self.node, initial_state=seed, peers=peers,
                                      execute_fn=state_machine)
        else:
            self.startup_role = bootstrap_cls(self.node,
                                      execute_fn=state_machine, peers=peers)
        self.requester = None

    def start(self):
        self.startup_role.start()
        self.thread = threading.Thread(target=self.network.run)
        self.thread.start()

    def invoke(self, input_value, request_cls=Requester):
        assert self.requester is None
        q = Queue.Queue()
        self.requester = request_cls(self.node, input_value, q.put)
        self.requester.start()
        output = q.get()
        self.requester = None
        return output
```

### 角色类

让我们逐个查看库中的每个角色类。

#### Acceptor

该Acceptor类实现协议接受者角色，因此必须存放其最近的决议的投票编码和所有实例决议的集合。然后，它根据协议响应Prepare和Accept消息。这是一个简短的类，很容易与协议进行比较。

对于接受者来说，Multi-Paxos看起来很像Simple Paxos，只不过在消息中增加了实例号。

```python
class Acceptor(Role):

    def __init__(self, node):
        super(Acceptor, self).__init__(node)
        self.ballot_num = NULL_BALLOT
        self.accepted_proposals = {}  # {slot: (ballot_num, proposal)}

    def do_Prepare(self, sender, ballot_num):
        if ballot_num > self.ballot_num:
            self.ballot_num = ballot_num
            # we've heard from a scout, so it might be the next leader
            self.node.send([self.node.address], Accepting(leader=sender))

        self.node.send([sender], Promise(
            ballot_num=self.ballot_num, 
            accepted_proposals=self.accepted_proposals
        ))

    def do_Accept(self, sender, ballot_num, slot, proposal):
        if ballot_num >= self.ballot_num:
            self.ballot_num = ballot_num
            acc = self.accepted_proposals
            if slot not in acc or acc[slot][0] < ballot_num:
                acc[slot] = (ballot_num, proposal)

        self.node.send([sender], Accepted(
            slot=slot, ballot_num=self.ballot_num))
```

#### Replica

Replica类是最复杂的角色类，因为它有一些密切相关的职责：

- 提出新的提议;
- 在确定提案时调用本地状态机;
- 追踪当前的领导者;
- 将新启动的节点添加到群集。

Replica创建新提议以响应来自客户端的Invoke消息，选择它认为是未使用的实例并向当前领导者发送Propose消息。此外，如果所选实例的共识对应着另外的的提议，则Replica必须重新提议使用新的实例。

![图片](http://aosabook.org/en/500L/cluster-images/replica.png)

Decision消息表示该实例中集群已达成共识。此时，Replica存储新的决议，然后运行状态机直到它到达未决议的实例。Replica需要区分开集群已通过决议的实例和来自本地状态机已提交处理的实例。当实例出现乱序的决议时，提交的提议可能会滞后以等待下一个实例被决议。提交实例时，Replica都会将Invoked消息发送回请求者并返回操作结果。

在某些情况下，某个实例可能没有生成任何有效的提议和决议。然后状态机需要逐个执行实例，因此集群必须就实例的内容达成共识。为了防止这种可能性，Replica只要遇到这样的实例就会提出“禁止操作”的提议。如果最终确定了这样的提议，那么状态机对该实例不进行任何操作。

同样地，同一提案可能会被决定两次。Replica跳过任何此类重复提议，不对该实例执行任何状态机转换。

Replica需要知道哪个节点是目前的领导者才能向其发送Propose消息。正如我们稍后会看到的那样，要实现这一目标需要大量精妙的细节设计。每个replica使用三个信息源跟踪当前的领导者。

当leader角色变为活动状态时，它会将Adopted消息发送到同一节点上的Replica。

![图片](http://aosabook.org/en/500L/cluster-images/adopted.png)

当Acceptor角色将Promise消息发送给新的领导者时，它会向其本地Replica发送一条Accepting消息。

![图片](http://aosabook.org/en/500L/cluster-images/accepting.png)

当前的领导者将Active消息作为心跳发送。如果在LEADER_TIMEOUT到期之前没有这样的消息到达，则Replica假定领导者已经死亡并继续LEADER_TIMEOUT到下一个领导者。在这种情况下，重要的是所有Replica选择相同的新领导者，我们通过对成员进行排序并选择列表中的下一个成员来完成。

![图片](http://aosabook.org/en/500L/cluster-images/active.png)

最后，当一个节点加入网络时，bootstrap角色会发送一条Join消息。Replica使用包含其最新状态的Welcome消息进行响应，从而使新节点快速上线。

![图片](http://aosabook.org/en/500L/cluster-images/bootstrap.png)

```python
class Replica(Role):

    def __init__(self, node, execute_fn, state, slot, decisions, peers):
        super(Replica, self).__init__(node)
        self.execute_fn = execute_fn
        self.state = state
        self.slot = slot
        self.decisions = decisions
        self.peers = peers
        self.proposals = {}
        # next slot num for a proposal (may lead slot)
        self.next_slot = slot
        self.latest_leader = None
        self.latest_leader_timeout = None

    # making proposals

    def do_Invoke(self, sender, caller, client_id, input_value):
        proposal = Proposal(caller, client_id, input_value)
        slot = next((s for s, p in self.proposals.iteritems() if p == proposal), None)
        # propose, or re-propose if this proposal already has a slot
        self.propose(proposal, slot)

    def propose(self, proposal, slot=None):
        """Send (or resend, if slot is specified) a proposal to the leader"""
        if not slot:
            slot, self.next_slot = self.next_slot, self.next_slot + 1
        self.proposals[slot] = proposal
        # find a leader we think is working - either the latest we know of, or
        # ourselves (which may trigger a scout to make us the leader)
        leader = self.latest_leader or self.node.address
        self.logger.info(
            "proposing %s at slot %d to leader %s" % (proposal, slot, leader))
        self.node.send([leader], Propose(slot=slot, proposal=proposal))

    # handling decided proposals

    def do_Decision(self, sender, slot, proposal):
        assert not self.decisions.get(self.slot, None), \
                "next slot to commit is already decided"
        if slot in self.decisions:
            assert self.decisions[slot] == proposal, \
                "slot %d already decided with %r!" % (slot, self.decisions[slot])
            return
        self.decisions[slot] = proposal
        self.next_slot = max(self.next_slot, slot + 1)

        # re-propose our proposal in a new slot if it lost its slot and wasn't a no-op
        our_proposal = self.proposals.get(slot)
        if (our_proposal is not None and 
            our_proposal != proposal and our_proposal.caller):
            self.propose(our_proposal)

        # execute any pending, decided proposals
        while True:
            commit_proposal = self.decisions.get(self.slot)
            if not commit_proposal:
                break  # not decided yet
            commit_slot, self.slot = self.slot, self.slot + 1

            self.commit(commit_slot, commit_proposal)

    def commit(self, slot, proposal):
        """Actually commit a proposal that is decided and in sequence"""
        decided_proposals = [p for s, p in self.decisions.iteritems() if s < slot]
        if proposal in decided_proposals:
            self.logger.info(
                "not committing duplicate proposal %r, slot %d", proposal, slot)
            return  # duplicate

        self.logger.info("committing %r at slot %d" % (proposal, slot))
        if proposal.caller is not None:
            # perform a client operation
            self.state, output = self.execute_fn(self.state, proposal.input)
            self.node.send([proposal.caller], 
                Invoked(client_id=proposal.client_id, output=output))

    # tracking the leader

    def do_Adopted(self, sender, ballot_num, accepted_proposals):
        self.latest_leader = self.node.address
        self.leader_alive()

    def do_Accepting(self, sender, leader):
        self.latest_leader = leader
        self.leader_alive()

    def do_Active(self, sender):
        if sender != self.latest_leader:
            return
        self.leader_alive()

    def leader_alive(self):
        if self.latest_leader_timeout:
            self.latest_leader_timeout.cancel()

        def reset_leader():
            idx = self.peers.index(self.latest_leader)
            self.latest_leader = self.peers[(idx + 1) % len(self.peers)]
            self.logger.debug("leader timed out; tring the next one, %s", 
                self.latest_leader)
        self.latest_leader_timeout = self.set_timer(LEADER_TIMEOUT, reset_leader)

    # adding new cluster members

    def do_Join(self, sender):
        if sender in self.peers:
            self.node.send([sender], Welcome(
                state=self.state, slot=self.slot, decisions=self.decisions))
```

#### Leader, Scout, and Commander

领导者的主要任务是采取Propose消息提出新的选票并做出决议。当成功执行协议的Prepare / Promise部分时，领导者是“活跃的”。活跃的领导者可以立即发送Accept消息以响应Propose 。

为了与每个角色的模型保持一致，领导者委派Scout和Commander角色来执行协议的每个部分。

```python
class Leader(Role):

    def __init__(self, node, peers, commander_cls=Commander, scout_cls=Scout):
        super(Leader, self).__init__(node)
        self.ballot_num = Ballot(0, node.address)
        self.active = False
        self.proposals = {}
        self.commander_cls = commander_cls
        self.scout_cls = scout_cls
        self.scouting = False
        self.peers = peers

    def start(self):
        # reminder others we're active before LEADER_TIMEOUT expires
        def active():
            if self.active:
                self.node.send(self.peers, Active())
            self.set_timer(LEADER_TIMEOUT / 2.0, active)
        active()

    def spawn_scout(self):
        assert not self.scouting
        self.scouting = True
        self.scout_cls(self.node, self.ballot_num, self.peers).start()

    def do_Adopted(self, sender, ballot_num, accepted_proposals):
        self.scouting = False
        self.proposals.update(accepted_proposals)
        # note that we don't re-spawn commanders here; if there are undecided
        # proposals, the replicas will re-propose
        self.logger.info("leader becoming active")
        self.active = True

    def spawn_commander(self, ballot_num, slot):
        proposal = self.proposals[slot]
        self.commander_cls(self.node, ballot_num, slot, proposal, self.peers).start()

    def do_Preempted(self, sender, slot, preempted_by):
        if not slot:  # from the scout
            self.scouting = False
        self.logger.info("leader preempted by %s", preempted_by.leader)
        self.active = False
        self.ballot_num = Ballot((preempted_by or self.ballot_num).n + 1, 
                                 self.ballot_num.leader)

    def do_Propose(self, sender, slot, proposal):
        if slot not in self.proposals:
            if self.active:
                self.proposals[slot] = proposal
                self.logger.info("spawning commander for slot %d" % (slot,))
                self.spawn_commander(self.ballot_num, slot)
            else:
                if not self.scouting:
                    self.logger.info("got PROPOSE when not active - scouting")
                    self.spawn_scout()
                else:
                    self.logger.info("got PROPOSE while scouting; ignored")
        else:
            self.logger.info("got PROPOSE for a slot already being proposed")
```

领导者在想要变为活动状态时创建Scout角色，以响应在其处于非活动状态时收到的Propose 。侦察员发送（并在必要时重新发送） Prepare消息，直到它从多数派那里接收到Promise响应或被抢占为止。它通过Adopted或Preempted消息与领导者通信。

![图片](http://aosabook.org/en/500L/cluster-images/leaderscout.png)

领导者为每个有活跃提案的实例创建一个Commander角色。跟Scout一样，Commander发送Accept消息并等待多数派Acceptor回复Accepted ，或收到抢占的消息。当提议被接受时，Commander向所有节点广播Decision消息。它通过Decided或Preempted响应领导者。

![图片](http://aosabook.org/en/500L/cluster-images/leadercommander.png)

```
class Commander(Role):

    def __init__(self, node, ballot_num, slot, proposal, peers):
        super(Commander, self).__init__(node)
        self.ballot_num = ballot_num
        self.slot = slot
        self.proposal = proposal
        self.acceptors = set([])
        self.peers = peers
        self.quorum = len(peers) / 2 + 1

    def start(self):
        self.node.send(set(self.peers) - self.acceptors, Accept(
            slot=self.slot, ballot_num=self.ballot_num, proposal=self.proposal))
        self.set_timer(ACCEPT_RETRANSMIT, self.start)

    def finished(self, ballot_num, preempted):
        if preempted:
            self.node.send([self.node.address], 
                           Preempted(slot=self.slot, preempted_by=ballot_num))
        else:
            self.node.send([self.node.address], 
                           Decided(slot=self.slot))
        self.stop()

    def do_Accepted(self, sender, slot, ballot_num):
        if slot != self.slot:
            return
        if ballot_num == self.ballot_num:
            self.acceptors.add(sender)
            if len(self.acceptors) < self.quorum:
                return
            self.node.send(self.peers, Decision(
                           slot=self.slot, proposal=self.proposal))
            self.finished(ballot_num, False)
        else:
            self.finished(ballot_num, True)
```

另外，在开发过程中出现了一个令人惊讶的小bug。当时，网络模拟器甚至在节点内的消息上引入了丢包。当*所有* `Decision`消息都丢失时，协议无法继续。replica继续重新发送`Propose`消息，但领导者忽略了它们，因为它已经有了该实例的提议。replica的追赶进程无法找到结果，因为没有replica听说过该决议。解决方案是确保始终传递本地消息，就像真实网络堆栈一样。

#### Bootstrap

当节点加入集群时，它必须先确定当前集群状态才能参与。Bootstrap角色依次向每个节点发送Join消息直到它收到Welcome。Bootstrap的通信图如上Replica所示 。

协议的早期版本为每个节点启动了一组完整的角色（Replica，Leader和Acceptor），每个角色都在“启动”阶段开始，等待Welcome消息中的信息。这将初始化逻辑分散在每个角色周围，需要对每个角色进行单独测试。最终设计具有引导角色，一旦启动完成，将每个其他角色添加到节点，将初始状态传递给它们的构造函数。

```python
class Bootstrap(Role):

    def __init__(self, node, peers, execute_fn,
                 replica_cls=Replica, acceptor_cls=Acceptor, leader_cls=Leader,
                 commander_cls=Commander, scout_cls=Scout):
        super(Bootstrap, self).__init__(node)
        self.execute_fn = execute_fn
        self.peers = peers
        self.peers_cycle = itertools.cycle(peers)
        self.replica_cls = replica_cls
        self.acceptor_cls = acceptor_cls
        self.leader_cls = leader_cls
        self.commander_cls = commander_cls
        self.scout_cls = scout_cls

    def start(self):
        self.join()

    def join(self):
        self.node.send([next(self.peers_cycle)], Join())
        self.set_timer(JOIN_RETRANSMIT, self.join)

    def do_Welcome(self, sender, state, slot, decisions):
        self.acceptor_cls(self.node)
        self.replica_cls(self.node, execute_fn=self.execute_fn, peers=self.peers,
                         state=state, slot=slot, decisions=decisions)
        self.leader_cls(self.node, peers=self.peers, commander_cls=self.commander_cls,
                        scout_cls=self.scout_cls).start()
        self.stop()
```

#### Seed

在正常操作中，当节点加入集群时，它期望找到已经运行的集群，其中至少有一个节点愿意响应Join消息。但是集群是如何开始的？一种选择是让引导角色在尝试联系每个其他节点后确定它是集群中的第一个。但这有两个问题。首先，对于大型集群而言，这意味着每次`Join`时都会等待很长时间。更重要的是，在网络分区的情况下，新节点可能无法联系任何其他节点并启动新集群。

网络分区是集群应用程序最具挑战性的故障情况。在网络分区中，所有集群成员都保持活动状态，但某些成员之间的通信失败。例如，如果加入集群的网络与柏林和台北的节点链接失败，则网络将被分区。如果集群的两个部分在分区期间继续运行，则在恢复网络链接之后重新连接部件可能具有挑战性。在Multi-Paxos案例中，已修复的网络将托管两个集群，其具有针对相同实例的不同决议。

为避免此结果，创建新集群是用户指定的操作。集群中只有一个节点运行Seed角色，其他节点像往常一样运行Bootstrap。Seed等待，直到它收到来自多数派的Join消息，然后发送一个Welcome其中包含状态机的初始状态和一组空的决议。然后，Seed角色将自行停止并启动Bootstrap角色以加入新生成的集群。

Seed模拟Bootstrap/Replica交互的Join / Welcome部分，因此其通信图与Replica角色相同。

```python
class Seed(Role):

    def __init__(self, node, initial_state, execute_fn, peers, 
                 bootstrap_cls=Bootstrap):
        super(Seed, self).__init__(node)
        self.initial_state = initial_state
        self.execute_fn = execute_fn
        self.peers = peers
        self.bootstrap_cls = bootstrap_cls
        self.seen_peers = set([])
        self.exit_timer = None

    def do_Join(self, sender):
        self.seen_peers.add(sender)
        if len(self.seen_peers) <= len(self.peers) / 2:
            return

        # cluster is ready - welcome everyone
        self.node.send(list(self.seen_peers), Welcome(
            state=self.initial_state, slot=1, decisions={}))

        # stick around for long enough that we don't hear any new JOINs from
        # the newly formed cluster
        if self.exit_timer:
            self.exit_timer.cancel()
        self.exit_timer = self.set_timer(JOIN_RETRANSMIT * 2, self.finish)

    def finish(self):
        # bootstrap this node into the cluster we just seeded
        bs = self.bootstrap_cls(self.node, 
                                peers=self.peers, execute_fn=self.execute_fn)
        bs.start()
        self.stop()
```

#### Requester

Requester角色管理对分布式状态机的请求。它只是将Invoke消息发送到本地Replica，直到它收到相应的Invoked 。有关此角色的通信图，请参阅上面的“Replica”部分。

```python
class Requester(Role):

    client_ids = itertools.count(start=100000)

    def __init__(self, node, n, callback):
        super(Requester, self).__init__(node)
        self.client_id = self.client_ids.next()
        self.n = n
        self.output = None
        self.callback = callback

    def start(self):
        self.node.send([self.node.address], 
                       Invoke(caller=self.node.address, 
                              client_id=self.client_id, input_value=self.n))
        self.invoke_timer = self.set_timer(INVOKE_RETRANSMIT, self.start)

    def do_Invoked(self, sender, client_id, output):
        if client_id != self.client_id:
            return
        self.logger.debug("received output %r" % (output,))
        self.invoke_timer.cancel()
        self.callback(output)
        self.stop()
```

### Summary

总结一下，集群的角色是：

- Acceptor - 做出承诺并接受提议
- Replica - 管理分布式状态机：提交提议，提交决策和响应Requester
- Leader - Multi-Paxos算法的主导者
- Scout - 为Leader执行Multi-Paxos算法的Prepare / Promise部分
- Commander - 为Leader执行Multi-Paxos算法的Accept / Accepted部分
- Bootstrap - 将新节点引入现有集群
- Seed - 创建一个新的集群
- Requester - 请求分布式状态机操作

现在使Cluster真正运行起来还需要一个部分：所有节点进行通信所需的网络。

## 网络

任何网络协议都需要能够发送和接收消息，以及调用功能的方法。

Network类提供了具有这些功能的简单模拟网络，还模拟了数据包丢失和消息传播延迟。

使用Python的heapq模块处理定时器，允许有效选择下一个事件。设置计时器涉及将Timer对象推送到堆上。由于从堆中删除项目效率低，因此取消的计时器保留在原位但标记为已取消。

消息传输使用定时器功能，使用随机模拟延迟来调度稍后在每个节点处传递消息。我们再次使用functools.partial设置将来使用适当参数调用目标节点的receive方法。

运行模拟只涉及从堆中弹出计时器，如果它们尚未被取消并且目标节点仍处于活动状态，则执行它们。

```python
class Timer(object):

    def __init__(self, expires, address, callback):
        self.expires = expires
        self.address = address
        self.callback = callback
        self.cancelled = False

    def __cmp__(self, other):
        return cmp(self.expires, other.expires)

    def cancel(self):
        self.cancelled = True


class Network(object):
    PROP_DELAY = 0.03
    PROP_JITTER = 0.02
    DROP_PROB = 0.05

    def __init__(self, seed):
        self.nodes = {}
        self.rnd = random.Random(seed)
        self.timers = []
        self.now = 1000.0

    def new_node(self, address=None):
        node = Node(self, address=address)
        self.nodes[node.address] = node
        return node

    def run(self):
        while self.timers:
            next_timer = self.timers[0]
            if next_timer.expires > self.now:
                self.now = next_timer.expires
            heapq.heappop(self.timers)
            if next_timer.cancelled:
                continue
            if not next_timer.address or next_timer.address in self.nodes:
                next_timer.callback()

    def stop(self):
        self.timers = []

    def set_timer(self, address, seconds, callback):
        timer = Timer(self.now + seconds, address, callback)
        heapq.heappush(self.timers, timer)
        return timer

    def send(self, sender, destinations, message):
        sender.logger.debug("sending %s to %s", message, destinations)
        # avoid aliasing by making a closure containing distinct deep copy of
        # message for each dest
        def sendto(dest, message):
            if dest == sender.address:
                # reliably deliver local messages with no delay
                self.set_timer(sender.address, 0,  
                               lambda: sender.receive(sender.address, message))
            elif self.rnd.uniform(0, 1.0) > self.DROP_PROB:
                delay = self.PROP_DELAY + self.rnd.uniform(-self.PROP_JITTER, 
                                                           self.PROP_JITTER)
                self.set_timer(dest, delay, 
                               functools.partial(self.nodes[dest].receive, 
                                                 sender.address, message))
        for dest in (d for d in destinations if d in self.nodes):
            sendto(dest, copy.deepcopy(message))
```

虽然它不包含在此协议中，但组件模型允许我们换到实际网络实现，在真实网络上的实际服务器之间进行通信，而不更改其他组件。可以使用模拟网络进行测试和调试，生成使用在真实网络硬件上运行的库。
